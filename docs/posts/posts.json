[
  {
    "path": "posts/2022-05-18-examining-targeted-ads/",
    "title": "Examining Targeted Ads",
    "description": "An Exercise from R.A.T. Discussion Guide",
    "author": [
      {
        "name": "Joshua Vong and Ethan Nguyen",
        "url": {}
      }
    ],
    "date": "2022-05-17",
    "categories": [],
    "contents": "\r\nAs part of the Advance Data Science Course at Chico State, we read the book “Race After Technology” by Ruha Benjamin in order to better understand the real-world impact data science systems can have. In the discussion guide for this book, Rachael Zafer creates the following activity:\r\n\r\nSet a timer for 10 minutes and open a social media app. Keep a tally of the number of ads you encounter during this period of time and the type of ads you see. Compare your list of targeted ads with a peer. What differences and similarities do you notice?\r\n\r\nThe following are our main takeaways having done this activity ourselves (as well as inviting three friends and family members for additional data points).\r\nData Collection and Exploration\r\nTo begin this exercise we asked everyone to browse through their most used social media site while keeping a running list of any ads they came across, including any duplicate ads. They then sent this list to us.\r\nAfter collecting the information from everyone, we created two graphs to help us visualize the types of ads that people were seeing. To do this, we created 12 broad categories of ads that we felt captured the industry / product type of the ads. Importing this information into Google sheets, we plotted these categories first with categories per person and then with categories grouped by gender. The raw data of the exercise is attached at the end.\r\n\r\n\r\nSimilarities\r\nA similarity in the ads was that there were a couple of ad categories that were “universal”. Food and drinks, and electronics / software were the main categories that every person seemed to get (though some got them a lot more in comparison to others). A specific ad that many got (3 out of 5) was for the movie “Doctor Strange in the Multiverse of Madness” which was releasing later that week.\r\nIn general, our findings comport with our understanding of targeted advertising; the types of ads that everyone seems to be seeing all appear to be the kinds of ads that would appeal to most people.\r\nDifferences\r\nThe more interesting part for us personally in this exercise was looking at the differences in the kinds of ads. The two specific examples are Person 1 and Person 4; no one came close to seeing the amount of clothing based ads as Person 1 and Person 4 had more than double the amount of electronic/software ads than the next highest person. Looking at our graph by gender, the same categories identified (clothing and electronics / software) are also the main categories, along with movies / tv / music, that are drastically different by gender.\r\nAnother interesting difference that we noticed was the number of ads that everyone was seeing, ranging from 0.7 to 2.4 ads per minute. We believe that this is mostly due to the app that is being used. The two highest ads per minute were from Instagram and Reddit; both sites have content that is meant to be consumed relatively quickly meaning users are likely to be scrolling past more ads compared to a site like YouTube where users are more “fixed” on the content.\r\nConclusion\r\nOverall we found this exercise to be very interesting; critically examining the ads that we are seeing was not something that we thought a lot about. Despite being aware, going into the exercise, that the internet tends to alter certain search results based on what it thinks we will personally engage with, it was eye opening to see just how well targeted these ads are.\r\nRaw Data\r\n\r\nBios\r\nJoshua Vong\r\nUndergraduate Computer Science major at Chico State in the Data Science Certificate Program\r\nhttps://data485-s22.github.io/website-distill-Jvong-max/\r\nEthan Nguyen\r\nUndergraduate Computer Science major at Chico State in the Data Science Certificate Program\r\nhttps://data485-s22.github.io/website-distill-ethanhn11/\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-18T16:18:23-07:00",
    "input_file": "examining-targeted-ads.knit.md"
  },
  {
    "path": "posts/2022-03-19-linear-regression-in-tidymodels/",
    "title": "Linear Regression in Tidymodels",
    "description": "A sample walkthrough using the mtcars dataset",
    "author": [
      {
        "name": "Ethan Nguyen",
        "url": "https://github.com/ethanhn11"
      }
    ],
    "date": "2022-03-19",
    "categories": [],
    "contents": "\r\nIn this blog post, I will walkthrough how to use the R library tidymodels in order to apply a linear regression model to a dataset. The dataset I will be using is called mtcars which is one of the many dataset that is built into R by default. mtcars contains observations of 32 cars that were built between 1973 and 1974; there are 11 variables that describe features of each observation such as its miles per gallon, its weight, the number of cylinders, etc. Side note: since mtcars is a built in dataset, running ?mtcars will bring up some useful documentation about the dataset.\r\nBefore we get started, we can conduct some exploratory data analysis and look at two variables, the number of cylinders (cyl) and the weight of the car (wt) with respect to the miles per gallon.\r\n\r\n\r\nggplot(data = mtcars) +\r\n  geom_point(mapping = aes(x = cyl, y = mpg))\r\n\r\n\r\n\r\nggplot(data = mtcars) +\r\n  geom_point(mapping = aes(x = wt, y = mpg))\r\n\r\n\r\n\r\n\r\nSince both of the variables looks relatively linear with respect to miles per gallon, we are free to proceed with building our linear regression model.\r\nSetup\r\nThe very first step to using tidymodels is to first import the tidymodels library into our work environment:\r\n\r\n\r\nlibrary(tidymodels)\r\n\r\n\r\n\r\nIf this is your first time working with tidymodels, remember to first install the package with install_package('tidymodels').\r\nAfter that, we will then create a regression model object known as the parsnip specification. This is done by calling the linear_reg() function and piping that into set_mode() as shown below:\r\n\r\n\r\nlm_spec <- linear_reg() %>%\r\n  set_mode(\"regression\")\r\n\r\n\r\n\r\nFitting the Model\r\nWith the parsnip specification set up, we can now fit the model by piping the model into the fit() function. This function takes a linear regression formula as the first argument and the dataset as a keyword argument.\r\nThe basic format of the formula for a linear regression is \\(y \\sim x_1 + x_2 + ... + x_i\\) where y is the explanatory variable and x are the response variables. For example, using the mtcars dataset, if we wanted to create a linear regression formula to predict a car’s mpg given the number of cylinders and the car’s weight, the formula would look like mpg ~ cyl + wt.\r\nApplying everything mentioned above we get:\r\n\r\n\r\nlm_fit <- lm_spec %>%\r\n  fit(mpg ~ cyl + wt, data = mtcars)\r\n\r\n\r\n\r\nNote that the fit() function returns another object, in the example, we are setting that returned object to the lm_fit variable.\r\nViewing and Interpreting the Model\r\nNow that we have the fitted model object, there are many different ways to view the specific values of the model. One of the easier ways to view the result is to use the tidy() function and passing in the fit object as an argument.\r\n\r\n\r\ntidy(lm_fit)\r\n\r\n\r\n# A tibble: 3 x 5\r\n  term        estimate std.error statistic  p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept)    39.7      1.71      23.1  3.04e-20\r\n2 cyl            -1.51     0.415     -3.64 1.06e- 3\r\n3 wt             -3.19     0.757     -4.22 2.22e- 4\r\n\r\nThis table means that the linear regression model to predict a car’s miles per gallon can be expressed as:\r\n\\[\\hat{mpg} = 39.7 - 1.51 * cyl - 3.19 * wt\\]\r\nTo get specifics on how well the model is fitted, we can use the glance() function:\r\n\r\n\r\nglance(lm_fit)\r\n\r\n\r\n# A tibble: 1 x 12\r\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\r\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\r\n1     0.830         0.819  2.57      70.9 6.81e-12     2  -74.0  156.\r\n# ... with 4 more variables: BIC <dbl>, deviance <dbl>,\r\n#   df.residual <int>, nobs <int>\r\n\r\nThe resulting \\(r^2\\) value means that the model we just fitted explains 83% of the variance in miles per gallon.\r\nConclusion\r\nAnd that’s it, we have successfully fitted a linear regression model to predict a car’s mpg! To briefly summarize the steps taken, we\r\nfirst, create a specification object\r\nthen called the fit() function and specified the regression formula\r\nand lastly, viewed the results using tidy() and glance()\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-19-linear-regression-in-tidymodels/linear-regression-in-tidymodels_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-03-23T14:32:03-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-10-something-i-found-interesting-the-elo-rating-system/",
    "title": "Something I Found Interesting: The Elo Rating System",
    "description": "Using the Elo system to predict the outcome of games",
    "author": [
      {
        "name": "Ethan Nguyen",
        "url": "https://github.com/ethanhn11"
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nSomething I found interesting that I recently learned about in the realm of data science is the Elo rating system and how while being relatively simple, it can be used to predict the outcome of a two-player game. I have always been vaguely familiar with the Elo scheme hearing the term here and there, but only recently did I learn a little bit about its inner workings.\r\nOverview\r\nThe Elo rating system was developed by Arpad Elo and was used as a way to measure how good a particular chess player is. The basic idea behind Elo is that each chess player gets assigned some starting points to represent their skill; if they win a match they take points from the opponent and if they lose they give up points to their opponents. The difference in points going into a match between two players is used to predict the outcome of the game. When two players with a different amount of points play, the player with the lower amount of points at the time of the match has the opportunity to gain more points. For example, if Player A was an underdog going into the match against Player B (i.e. A has fewer points than B), if Player A wins, A might take 32 points from B while if B wins, B might only take 10 points from A.\r\nWhile it was initially developed for chess, the Elo rating system has been applied more broadly to any two-player game as a way to rank how good each player is and to predict the outcome of matches. The one application of the Elo system that got me interested in learning more about it was FiveThirtyEight’s NFL predictions, which uses a slightly modified version of the Elo system to predict the outcome of football games.\r\nImplementation\r\nThis section is a summary of singingbanana’s youtube video on the Elo system.\r\nBaked into the Elo calculations is the idea that a difference of 400 points between players means that a player is 10 times more likely to win, and an 800 point difference means that a player is 100 times more likely to win.\r\nExpressed as a formula, the probability of a player A winning is written as \\[P(A \\ wins) = \\frac{1}{1 + 10^{\\frac{R_B - R_A}{400}}}\\] where \\(R_B\\) and \\(R_A\\)is the ranking (or amount of points) of player B and player A respectively. Using this formula, the Elo system can be used to predict the outcome of a game; e.g. if player A and player B have the same ranking, the probability of A winning 1/2.\r\nThe second component of the Elo system is how to update each player’s ranking after the game. This formula is expressed as: \\[new \\ rating = old \\ rating + 32(score - expected \\ score)\\] where the score is the result of the game (0 for loss, 1 for win) and the expected score was the previously calculated probability that the player would win. The 32 in this formula represents the maximum amount of points a player could win/lose in a game and is an “arbitrary” choice.\r\nLimitation\r\nThe simplistic nature of using the Elo system means that when using it as a model to predict the outcome of games, a lot of potentially useful variables are going to be ignored. Without making any modification to the “base” Elo system, the only thing that Elo is using to make predictions is the history of matches of both players.\r\nSome modifications can be used to enhance Elo as a predictive model. FiveThirtyEight details some of their modifications to their NFL Elo predictions here. One that I find particularly interesting is the home-field adjustment which gives the home time additional Elo points based on how far the visiting team had to travel.\r\nConclusion\r\nIn conclusion, the Elo rating system can be used as a model to predict the outcome of any two-player game. Though some modifications can be made to the base formulas, Elo remains a relatively simple model that mostly uses the history of players’ performance to predict future performances.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-11T12:34:47-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-01-hello-world/",
    "title": "Hello World",
    "description": "This is the first post and a short description of the post.",
    "author": [
      {
        "name": "Ethan Nguyen",
        "url": "https://example.com/"
      }
    ],
    "date": "2022-02-01",
    "categories": [],
    "contents": "\r\nDistill is a publication format for scientific and technical writing, native to the web.\r\nLearn more about using Distill at https://rstudio.github.io/distill.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-02-01T09:54:52-08:00",
    "input_file": {}
  }
]
